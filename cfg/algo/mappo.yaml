name: mappo

# ppo
train_every: 64
num_minibatches: 16
ppo_epochs: 4
clip_param: 0.1
entropy_coef: 0.001 # < 0 to force a concentrated policy
gae_lambda: 0.95
gamma: 0.995
max_grad_norm: 10.0
normalize_advantages: True

reward_weights: null # null means all 1.0
share_actor: true
critic_input: obs # `obs` or `state`

actor:
  lr: 0.0005
  lr_scheduler: # ExponentialLR
  lr_scheduler_kwargs: 
    gamma: 0.9985
  
  # mlp
  hidden_units: [256, 256, 256]
  layer_norm: true

  weight_decay: 0.
  gain: 0.01

  vision_encoder: MobileNetV3Small
  attn_encoder: PartialAttentionEncoder # if applicable
  use_orthogonal: true

  tanh: false
  attention_type: 3
  attention_dim: 32
  self_attention: true

  use_spectral_norm: false

  # rnn:
  #   cls: gru
  #   kwargs:
  #     hidden_size: 128
  #   train_seq_len: 16

critic:
  num_critics: 1
  use_popart: false
  value_norm:
    class: ValueNorm1
    kwargs: 
      beta: 0.995

  lr: 0.0005
  lr_scheduler: # ExponentialLR
  lr_scheduler_kwargs: 
    gamma: 0.9985

  # mlp
  hidden_units: [256, 256, 256]
  layer_norm: true

  weight_decay: 0.
  gain: 0.01

  use_huber_loss: true
  huber_delta: 10

  # rnn:
  #   cls: gru
  #   kwargs:
  #     hidden_size: 128
  #   train_seq_len: 16

  vision_encoder: MobileNetV3Small
  attn_encoder: PartialAttentionEncoder # if applicable
  use_feature_normalization: true
  use_orthogonal: true
  attention_type: 3
  attention_dim: 32
  self_attention: true
